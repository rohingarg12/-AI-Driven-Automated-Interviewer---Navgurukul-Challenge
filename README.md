# ğŸ“ AI-Driven Automated Interviewer for Project Presentations

An AI system that listens to a student presenting a project (screen share + speech) and conducts an adaptive interview based on content and responses.

![Tech Stack](https://img.shields.io/badge/Next.js-15-black?style=flat-square&logo=next.js)
![Groq](https://img.shields.io/badge/Groq-API-orange?style=flat-square)
![Llama](https://img.shields.io/badge/Llama-3.3%2070B-blue?style=flat-square)
![Tesseract](https://img.shields.io/badge/Tesseract.js-OCR-green?style=flat-square)
![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)

## ğŸ¯ Features

- **Screen Capture & OCR** - Tesseract.js extracts text/code from your screen in real-time
- **AI Vision Analysis** - Llama 4 Scout understands UI, diagrams, and architecture
- **Speech-to-Text** - Real-time transcription using Groq Whisper (whisper-large-v3-turbo)
- **Resume Upload with OCR** - Tesseract.js extracts skills and experience from resume images
- **Smart Question Generation** - Context-aware questions generated by Llama 3.3 70B
- **Text-to-Speech** - Natural voice output using Web Speech API
- **Comprehensive Evaluation** - Scoring on technical depth, clarity, originality, implementation, and communication
- **PDF Report** - Downloadable evaluation report

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ installed
- Groq API key (free at [console.groq.com](https://console.groq.com))

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/rohingarg12/-AI-Driven-Automated-Interviewer---Navgurukul-Challenge.git
cd -AI-Driven-Automated-Interviewer---Navgurukul-Challenge

# 2. Install dependencies
npm install

# 3. Set up environment
cp .env.example .env.local
# Edit .env.local and add your GROQ_API_KEY

# 4. Run the development server
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) in your browser.

## ğŸ“ Project Structure

```
navgurukul-interviewer/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ analyze/         # Screen analysis (Llama 4 Scout Vision)
â”‚   â”‚   â”œâ”€â”€ chat/            # Question generation (Llama 3.3 70B)
â”‚   â”‚   â”œâ”€â”€ evaluate/        # Interview evaluation
â”‚   â”‚   â”œâ”€â”€ resume/          # Resume processing (PDF + Groq Vision fallback)
â”‚   â”‚   â”œâ”€â”€ transcribe/      # Speech-to-text (Whisper)
â”‚   â”‚   â””â”€â”€ tts/             # Text-to-speech config
â”‚   â”œâ”€â”€ setup/               # Phase 0: Resume upload + OCR
â”‚   â”œâ”€â”€ presentation/        # Phase 1: Screen share + recording + OCR
â”‚   â”œâ”€â”€ interview/           # Phase 2: Q&A session
â”‚   â”œâ”€â”€ evaluation/          # Phase 3: Results & report
â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”œâ”€â”€ page.tsx             # Landing page
â”‚   â””â”€â”€ globals.css
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ui/                  # Button, Card, Badge, Progress
â”‚   â””â”€â”€ interview/           # ScreenCapture, VoiceRecorder, ResumeUpload, LiveCaptureDisplay
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ groq.ts              # Groq API client & prompts
â”‚   â”œâ”€â”€ store.ts             # Zustand state management
â”‚   â”œâ”€â”€ useClientOCR.ts      # ğŸ”¥ Tesseract.js OCR hook (client-side)
â”‚   â””â”€â”€ utils.ts             # Utility functions
â”œâ”€â”€ .env.example
â”œâ”€â”€ package.json
â”œâ”€â”€ tailwind.config.js
â””â”€â”€ README.md
```

## ğŸ”§ Tech Stack

| Component | Technology | Why |
|-----------|------------|-----|
| **Framework** | Next.js 15 + React 19 | Modern, fast, App Router |
| **Styling** | Tailwind CSS | Beautiful dark theme |
| **State** | Zustand | Simple, no boilerplate |
| **LLM** | Groq (Llama 3.3 70B) | Fast inference, free tier |
| **Vision** | Groq (Llama 4 Scout) | Understands UI & diagrams |
| **OCR** | Tesseract.js | ğŸ”¥ Client-side text extraction from images |
| **STT** | Groq Whisper | 95%+ accuracy, FREE |
| **TTS** | Web Speech API | Browser built-in, FREE |
| **PDF** | jsPDF | Client-side generation |

## ğŸ” OCR Implementation Details

We use **Tesseract.js** for OCR (Optical Character Recognition), running entirely in the browser:

### Where OCR is Used:

| Feature | File | Description |
|---------|------|-------------|
| **Resume Upload** | `components/interview/ResumeUpload.tsx` | Extracts text from resume images (PNG/JPG) |
| **Screen Capture** | `app/presentation/page.tsx` | Extracts code/text from shared screen |
| **OCR Hook** | `lib/useClientOCR.ts` | Reusable React hook for Tesseract.js |

### Why Client-Side OCR?

1. **No Server Crashes** - Tesseract.js workers don't work in Next.js server environment
2. **Faster** - No network round-trip, runs in browser WebAssembly
3. **Free** - No API costs, unlimited usage
4. **Privacy** - Images never leave the user's browser

### OCR Code Example:

```typescript
// lib/useClientOCR.ts
import Tesseract from 'tesseract.js';

export function useClientOCR() {
  const runOCR = async (imageSource: string | Blob) => {
    const result = await Tesseract.recognize(imageSource, 'eng', {
      logger: (m) => console.log(m.progress) // Progress updates
    });
    return result.data.text;
  };
  return { runOCR };
}
```

## ğŸ’° Cost: $0

Everything is **FREE**:
- âœ… Groq API: Free tier (30 requests/minute)
- âœ… Tesseract.js: Open source, runs in browser
- âœ… Web Speech API: Browser built-in
- âœ… Screen Capture: Browser built-in
- âœ… Hosting: Vercel free tier

## ğŸ® How It Works

### Phase 0: Setup (Optional)
1. Upload your resume (PDF, PNG, JPG, TXT)
2. **Tesseract.js OCR** extracts text from image
3. AI detects skills, education, experience
4. Click "Start with Resume" or "Skip"

### Phase 1: Presentation (3-5 minutes)
1. Click "Share Screen" and select your project
2. Click "Start Presentation"
3. Explain your project verbally
4. Every 5 seconds when screen changes:
   - ğŸ“¸ Screenshot captured
   - ğŸ” **Tesseract.js OCR** extracts text/code
   - ğŸ¤– Llama 4 Scout analyzes visual content
   - ğŸ¤ Whisper transcribes your speech
5. Click "End & Start Interview"

### Phase 2: Interview (5-10 minutes)
1. AI generates 5 questions based on:
   - Your resume (if uploaded)
   - OCR text from screen captures
   - AI vision analysis
   - Your verbal presentation
2. AI speaks each question (TTS)
3. You respond verbally
4. AI asks adaptive follow-up questions

### Phase 3: Evaluation
1. AI analyzes all Q&A pairs
2. Scores on 5 criteria (1-10 each)
3. Provides detailed feedback
4. Download PDF report

## ğŸ“Š Evaluation Criteria

| Criteria | Description | Weight |
|----------|-------------|--------|
| **Technical Depth** | Understanding of implementation details | 20% |
| **Clarity** | Ability to explain concepts clearly | 20% |
| **Originality** | Unique aspects and creative solutions | 20% |
| **Implementation** | Code quality and architecture | 20% |
| **Communication** | Overall presentation skills | 20% |

## ğŸ” Environment Variables

```env
# Required - Get from https://console.groq.com
GROQ_API_KEY=gsk_your_api_key_here
```

## ğŸ§— Challenges We Faced & Solved

### 1. Tesseract.js Server Crash
**Problem:** Tesseract.js uses Web Workers that crash in Next.js API routes
```
Error: Cannot find module '.next/worker-script/node/index.js'
```
**Solution:** Moved OCR to client-side using `useClientOCR` hook

### 2. Deprecated Groq Vision Model
**Problem:** `llama-3.2-90b-vision-preview` was decommissioned
**Solution:** Updated to `meta-llama/llama-4-scout-17b-16e-instruct`

### 3. Infinite Re-render Loop
**Problem:** Screen capture callbacks caused "Maximum update depth exceeded"
**Solution:** Used refs to store callbacks instead of direct props

### 4. C++ Skill Detection Regex Error
**Problem:** `new RegExp('\bC++\b')` fails because `+` is special
**Solution:** Escape special characters: `skill.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')`

### 5. Audio Transcription Empty File
**Problem:** Groq Whisper returned 400 for small audio chunks
**Solution:** Added minimum blob size check (skip if < 1000 bytes)

## ğŸš€ Deployment

### Vercel (Recommended)

```bash
npm install -g vercel
vercel
```

Add `GROQ_API_KEY` in Vercel Environment Variables.

### Local Production Build

```bash
npm run build
npm start
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing`)
5. Open a Pull Request

## ğŸ“„ License

MIT License - feel free to use for any purpose.

---

## ğŸ™ Acknowledgments

- [Navgurukul](https://navgurukul.org/) - AI/ML Challenge
- [Groq](https://groq.com/) - Lightning-fast LLM inference
- [Tesseract.js](https://tesseract.projectnaptha.com/) - Browser OCR


---

<p align="center">
  Built with â¤ï¸ for <strong>Navgurukul AI/ML Challenge 2025</strong>
</p>

<p align="center">
  Powered by Groq â€¢ Llama 3.3 & 4 Scout â€¢ Tesseract.js â€¢ Whisper â€¢ Web Speech API
</p>
